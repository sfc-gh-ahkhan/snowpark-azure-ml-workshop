{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imported Libraries\n",
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "# Classifier Libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Other Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Snowpark\n",
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark.functions import udf\n",
    "from snowflake.snowpark.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark import version as v\n",
    "print(v.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PASTE THE CONNECTION CODE BELOW PROVIDED BY INSTRUCTOR ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.add_packages('snowflake-snowpark-python', 'scikit-learn', 'pandas', 'numpy', 'joblib', 'cachetools')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(session.sql(\"select current_warehouse(), current_database(),current_schema(), current_role()\").collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use plain SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowdf = session.sql(\"select AMOUNT, AMOUNT*2 as DBL_AMOUNT from anomaly_base\")\n",
    "snowdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to Snowpark DF to Pandas DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = snowdf.to_pandas()\n",
    "pandas_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/pandas_df.png\" width=\"1000\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Snowpark, I can now create a Snowflake native dataframe and implement my logic in Python Snowpark Dataframes rather than SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowdf = session.table(\"anomaly_base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/snowdf.png\" width=\"1000\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform the same operation using Python dataframe operations that transpile down to SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowdf.select(\n",
    "    F.col('AMOUNT'),\n",
    "    (F.col('AMOUNT')*2).alias('DBL_AMOUNT')\n",
    ").toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowdf.select(\n",
    "    F.col('AMOUNT'),\n",
    "    (F.col('AMOUNT')*2).alias('DBL_AMOUNT')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I can specify predicates using the filter method or other commonly available dataframe API methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowdf.filter(F.col('CLASS')==1).limit(5).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I can also define my custom logic as a Python UDF that I can push down into Snowflake and then call using SQL or Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_it(x: int) -> int:\n",
    "    return 2*x\n",
    "\n",
    "double_udf = udf(double_it, name=f\"double_it_{user_number}\", replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowdf.select(F.col('AMOUNT'), double_udf(F.col('AMOUNT'))).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "#### Pandas provides many methods that help in EDA but is limited in scale because it doesn't scale beyond a single node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = snowdf.toPandas()\n",
    "pandas_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With the Snowpark DF, you are no longer limited by memory limitations as this `snowdf` exists in Snowflake and not in the memory of this notebook environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowdf.describe().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now lets take a look at our credit transaction data set..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('No Frauds', (snowdf.filter(F.col('CLASS')==0).count() / snowdf.count() * 100), '% of the dataset')\n",
    "print('Frauds', (snowdf.filter(F.col('CLASS')==1).count() / snowdf.count() * 100), '% of the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how imbalanced is our original dataset! Most of the transactions are non-fraud. If we use this dataframe as the base for our predictive models and analysis we might get a lot of errors and our algorithms will probably overfit since it will \"assume\" that most transactions are not fraud. But we don't want our model to assume, we want our model to detect patterns that give signs of fraud!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since our classes are highly skewed we should make them equivalent in order to have a normal distribution of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets shuffle the data before creating the subsamples\n",
    "\n",
    "pandas_df = pandas_df.dropna()\n",
    "df = pandas_df.sample(frac=1)\n",
    "\n",
    "# amount of fraud classes 492 rows.\n",
    "fraud_df = df.loc[df['CLASS'] == 1]\n",
    "non_fraud_df = df.loc[df['CLASS'] == 0][:492]\n",
    "\n",
    "normal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n",
    "\n",
    "# Shuffle dataframe rows\n",
    "new_df = normal_distributed_df.sample(frac=1, random_state=42)\n",
    "new_df.drop(['TIME', 'AMOUNT'], axis=1, inplace=True)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we have our dataframe correctly balanced, we can go and train a regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Logistic Regression Model\n",
    "\n",
    "#### Lets split up the features and the label first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_df.drop('CLASS', axis=1)\n",
    "y = new_df['CLASS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And split our training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets create a test dataset along with the CLASS variable that we can use to test our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = X_test\n",
    "combined_df['CLASS'] = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save this test data into Snowflake where we want to run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# First create the table\n",
    "list_val = [i+ \" number\" for i in combined_df.columns]\n",
    "query = f\"\"\" create or replace table ANOMALY_TEST_{user_number} ({','.join(map(str, list_val))})\"\"\"\n",
    "session.sql(query).collect()\n",
    "\n",
    "combined_df.columns = [i.upper() for i in combined_df.columns]\n",
    "\n",
    "# Write the data from the DataFrame to the table named \"TRIPS_FORECAST\".\n",
    "session.write_pandas(combined_df, table_name=f\"ANOMALY_TEST_{user_number}\", auto_create_table=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.table(f\"ANOMALY_TEST_{user_number}\").toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Turn the values into an array for feeding the classification algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(X_train.columns)\n",
    "\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a model using the LogisticRegression algorithm provide by Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(X, y)\n",
    "training_score = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "print(\"Classifiers: \", clf.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an inference UDF using Snowpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def predict_anomaly_lr(V1: float, V2: float, V3: float, V4: float, \n",
    "                    V5: float, V6: float, V7: float, V8: float, \n",
    "                    V9: float, V10: float, V11: float, V12: float, \n",
    "                    V13: float, V14: float, V15: float, V16: float, \n",
    "                    V17: float, V18: float, V19: float, V20: float, \n",
    "                    V21: float, V22: float, V23: float, V24: float,\n",
    "                    V25: float, V26: float, V27: float, V28: float) -> int:\n",
    "    \"\"\"Inferring types from type hints\"\"\"\n",
    "    row = pd.DataFrame([locals()], columns=features)\n",
    "    return clf.predict(row)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And push this inference function into Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_anomaly_udf = udf(predict_anomaly_lr, name=f\"model_{user_number}\", replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we run inference against the test data we previously saved into the `ANOMALY_PREDICTIONS` table in Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_snowdf = session.table(f\"ANOMALY_TEST_{user_number}\")\n",
    "inputs = test_snowdf.drop(F.col('CLASS'))\n",
    "# Score the test data - which we know are all anomalies\n",
    "prediction_snowdf = test_snowdf.select(*inputs,\n",
    "                    predict_anomaly_udf(*inputs).alias('PREDICTION'), \n",
    "                    (F.col('CLASS')).alias('ACTUAL_LABEL')\n",
    "                    ).limit(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notice that this üëÜüèº inference is being run in Snowflake as a UDF and NOT in this notebook. This makes it a scalable inference pipeline which can automatically parallelize the inference as you increase the size of the warehouse in Snowflake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prediction_snowdf.toPandas()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally, lets plot the actual labels and what our model predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x=df.index, y=\"ACTUAL_LABEL\")\n",
    "sns.scatterplot(data=df, x=df.index, y=\"PREDICTION\")\n",
    "\n",
    "plt.legend(labels=['ACTUAL_LABEL', 'PREDICTION'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the predictions into Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# First create the table\n",
    "list_val = [i+ \" number\" for i in combined_df.columns]\n",
    "query = f\"\"\" create or replace table ANOMALY_PREDICTION_{user_number} ({','.join(map(str, list_val))})\"\"\"\n",
    "session.sql(query).collect()\n",
    "\n",
    "prediction_snowdf.write.mode(\"overwrite\").saveAsTable(f\"ANOMALY_PREDICTION_{user_number}\") #push to snowflake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> References: </h2>\n",
    "<ul>\n",
    "    <li>Adopted from this <a href=\"https://www.kaggle.com/janiobachmann/credit-fraud-dealing-with-imbalanced-datasets\">Kaggle notebook</a></li>\n",
    "<li>Hands on Machine Learning with Scikit-Learn & TensorFlow by Aur√©lien G√©ron (O'Reilly). CopyRight 2017 Aur√©lien G√©ron  </li>\n",
    "<li><a src=\"https://www.youtube.com/watch?v=DQC_YE3I5ig&t=794s\" > Machine Learning - Over-& Undersampling - Python/ Scikit/ Scikit-Imblearn </a>by Coding-Maniac</li>\n",
    "<li><a src=\"https://www.kaggle.com/lane203j/auprc-5-fold-c-v-and-resampling-methods\"> auprc, 5-fold c-v, and resampling methods\n",
    "</a> by Jeremy Lane (Kaggle Notebook) </li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "hex_info": {
   "author": "Miles Adkins",
   "exported_date": "Fri Jan 20 2023 04:21:44 GMT+0000 (Coordinated Universal Time)",
   "project_id": "38ac5681-ea33-40da-b5c5-3770e9e15964",
   "version": "draft"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f93cf183baa6bcb7087a6f33dce48bec9dc526703a537a3ec5d66d55e0208538"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
